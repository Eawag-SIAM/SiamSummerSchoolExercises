---
title: 'Exercise 5: Approximative Bayesian Computation (ABC)'
author: "Eawag Summer School in Environmental Systems Analysis"
output:
  html_document:
    number_sections: no
    toc: TRUE
    toc_depth: 3
    toc_float: true
    css: ../eawag.css
params:
  showsolutions: TRUE
---

<!-- setup Julia environmental and JS Code -->
```{r, child = "../header.Rmd"}
```


Objectives

* Understand when to use ABC and when not.
* Understand the fundamental difference between ABC and likelihood-based inference.
* Learn how to use the `SABC` function in the [`EasyABC`](https://cran.r-project.org/web/packages/EasyABC/index.html) package.
* Understand why and how to use summary statistics in ABC.

In this exercise, we run the Simulated Annealing ABC (SABC), for a Bayesian inference of the parameter of the model "survival", with a flat prior. In practice, we wouldn't use an ABC algorithm for this problem (why?).

# Define prior and a function to sampler from the model

We use the R package [`EasyABC`](https://cran.r-project.org/web/packages/EasyABC/index.html), which contains multiple sampling schemes for Approximate Bayes Computations. In addition, we need two other packages for some auxiliary tasks.
```{r message=FALSE, warning=FALSE}
library(EasyABC)
library(FME)  # for adaptive Metropolis algorithm
```

As a next step, let's read the data that we observed in the form of
death counts per observation interval, and which we try to explain by
the model survival. The experiment was started with `N=30` individuals.
```{r}
obs <- read.table("../../data/model_survival.csv", header=T)
head(obs)
```

For ABC, we always need a function that evaluates the prior density, a
function that samples from the prior, and a function that generates
random model outputs (our model). **Note that we do not need a
function which evaluates the density of the likelihood**.
The functions that we need are defined in the following:
```{r}
d.prior <- function(par) dunif(par, min=0.01, max=0.6) # density of prior
r.prior <- function() runif(1, min=0.01, max=0.6)      # sampler from prior

## produce model output
survival.sampler <- function(par, t=obs$time, N=30){
    if(par<0) return(rep(0,length(t)))  # return "no deaths" if par is negative:
    S <- exp(-par*c(0,t))               # and add zero "at time-point infinity"
    S <- c(S,0)
    p <- -diff(S)                       # probabilities to die within observation windows
    y <- rmultinom(1, N, p)[-(length(t)+1) ] # sample from multinomial distribution
                                             # (assume that there are N individuals
                                             #  at the beginning of the experiment)
    return(y)
}


```


# 1. ABC without summary statistics {#task1}

Use the `SABC` function of the `EasyABC` package, without summary
statistics, to get an approximate sample from the posterior.

Remember that in ABC, we need a distance measure to compare the model
output to the observations, in order to judge the plausibility of the
parameters that generated this model output. Since we do not apply
summary statistics in Task 1, we compare the model output and the
observations directly in the original data space:

```{r}
f.dist <- function(par) {
    sum(abs(survival.sampler(par) - obs$deaths))
}
```

Now we can run the SABC without summary statistics, using the distance in the original space given by `f.dist`:
```{r results='hide'}
SABC.res <- SABC(
  r.model       = f.dist,  # function that directly returns scalar
                                        # distance between a random model output and data
  r.prior       = r.prior, # sampler from prior
  d.prior       = d.prior, # density evaluator of prior
  n.sample      = 2000,
  eps.init      = 20,
  iter.max      = 60000,
  v             = 1.2,
  beta          = 0.8
)
```

# 2. ABC with summary statistics {#task2}

Use the `SABC` function, with semi-automatically generated summary statistics, to get an approximate sample from the posterior. "Semi-automatically" means that the user needs to provide the function `f.summarystats` that transforms model outputs. The algorithm then employs a linear regression from the transformed outputs to the model parameters to define the summary statistics (Fearnhead and Prangle (2012)). For this exercise, use the identity `f.summarystats(y)=y`. This means that we use a standard linear regression to estimate the parameters from the output and use these estimators as summary statistics.
```{r results='hide'}
f.summarystats <- function(y) {y}

SABC.SS.res <- SABC(
  r.model       = survival.sampler, # function that returns model output
  r.prior       = r.prior,           # sampler from prior
  d.prior       = d.prior,           # density evaluator of prior
  n.sample      = 2000,
  eps.init      = 20,
  iter.max      = 60000,
  summarystats  = TRUE,
  y             = obs$deaths,
  f.summarystats= f.summarystats,
  v             = 1.2,
  beta          = 0.8
)

```

# 3. Compare the previously generated posterior samples to a sample from the true posterior

In this toy exercise, the true posterior is available in closed form
and we can sample directly from it. This allows us to compare the two
samples generated in the previous tasks to a sample of the true
posterior. To generate a sample of the true posterior, we first define
the posterior function (see Monday excerises):
```{r}
logposterior <- function(y, N=30, t, par){
    log.prior <- log(d.prior(par))
    if(!is.finite(log.prior)) return(-Inf)

    ## Calculate survival probabilities at measurement points
    ## and add zero "at time-point infinity":
    S <- exp(-par*c(0,t))
    S <- c(S,0)
    ## Calcute probabilities to die within observation windows:
    p <- -diff(S)
    ## Add number of survivors at the end of the experiment:
    y <- c(y, N-sum(y))
    ## Calculate log-likelihood of multinomial distribution at point y:
    LL <- dmultinom(y, prob=p, log=TRUE)

    return(LL + log.prior)
}
```

In the following, we use an adaptive Metropolis algorithm to generate a sample from the true posterior:
```{r}
par.start <- c("lambda"=0.5)
jump.cov  <- 0.01
ntrydr     <- 3       # number of trials for Delayed Rejection
acc.rate   <- 0.23    # Desired acceptance rate for the Robust Adaptive Metropolis
iterations <- 10000   # length of the chain

## Run the Adaptive Metropolis Algorithm with Delayed Rejection:
obj.funct <- function(par, y, t){ -2*logposterior(y, N=30, t, par) }

AMDR <- modMCMC(
    f         = obj.funct,
    p         = par.start,
    jump      = jump.cov,
    niter     = 10000,
    updatecov = 10,
    covscale  = 2.4^2,
    ntrydr    = ntrydr,
    y = obs$deaths,
    t = obs$time
)

## Cut off burn-in and adaptation phase:
AMDR.chain  <- AMDR$pars[-(1:1000),]
```

Let us now compare the posterior samples generated with ABC with and
without summary statistics, and the sample of the true posterior. In
first Figure below, you can see that not using summary statistics can
lead to slow convergence: many proposals will be rejected since in
high dimensions it is very unlikely to match the data with sufficient
accuracy. However, the sample seems to converge to the true
distribution. The lower Figure shows that summary statistics speed up convergence, but this comes at the cost of a loss of information, which leads to biased results. The more of the information w.r.t. the parameters is captured by the summary statistics, the smaller is the resulting bias.
```{r wosummary, fig.cap = "True posterior (line) and sample generated via ABC **without** summary statistics (histogram)."}
plot(density(AMDR.chain), xlab=expression(lambda), main="")
hist(SABC.res$E[,1], add=TRUE, probability=TRUE,
     breaks=50, col=rgb(0,0,1,1/4))
```
```{r wsummary, fig.cap = "True posterior (line) and sample generated via ABC **with** summary statistics (histogram)."}
plot(density(AMDR.chain), xlab=expression(lambda), main="")
hist(SABC.SS.res$E[,1], add=TRUE, probability=TRUE,
     breaks=100, col=rgb(0,0,1,1/4))
```
