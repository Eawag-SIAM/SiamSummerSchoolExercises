---
title: 'Solution 1: Probabilistic models and likelihood functions'
author: "Eawag Summer School in Environmental Systems Analysis"
date: "24.5.2019"
output:
  html_document:
    number_sections: yes
  pdf_document: default
---



**Required Packages**:

In this exercise, we will need a couple of packages, including the `EawagSchoolTools`. 

```{r eval=FALSE}

library(lpridge)
library(lokern)
library(deSolve)
library(sensitivity)

library(EawagSchoolTools)

```

You can run the script `install_packages.r` (in the Dropbox directory `/R/`) to install and load all of them, if you haven't done so yet.




**Toy Models**:

Some exercises will use the toy models "Monod", "Growth", or "Survival", which were introduced in the previous lectures. You can see their definitions for instance in the slides "Mathematical Representation and Construction of Models" (Intro_Models.pdf).


**Full solution**:

The full solution R script is provided in [this R-script](./exercise_1_likelihood_sensitivity.R).


# Likelihood for a linear model (recommended) {#task1}

## Analytical expression

For a one-dimensional linear model with an additive error, a model output  $Y$ is given by the deterministic term $y_{det}$ plus the noise term. For the noise term we choose here identical independent normal errors, so that for a single observation $i$ (in a set of $n$ observations) the full model is given by

$$
Y_i = y_{i,det} + Z_i  , \quad \quad Z_i \sim \mathcal{N}(0,\sigma) , \quad \quad i \in \{1,\dots,n\}
\;.
$$


For the deterministic part, we have an input $x$ and two model parameters $\{\beta, \gamma\}$, from which $y_{det}$ is given by

$$
y_{det}(x,\beta, \gamma) =  \beta  x + \gamma
\;.
$$


We therefore have three model parameters in total: $\beta$ and $\gamma$ from the deterministic term, and $\sigma$ from the noise term. For convenience, we collect these into the set of parameters ${\boldsymbol{\theta}} = \{\beta,\gamma,\sigma\}$.


So for a single observation,  the model is given by

$$
Y_i = \beta  x_i + \gamma +Z_i 
\;,
$$


which is equivalent to

$$
Y_i \sim \mathcal{N}\left(y_{i,det}(x_i,\beta,\gamma),\sigma\right)
\;.
$$


That is, the model output $Y_i$ is normally distributed, where the mean is the deterministic model output $y_{i,det}$ and the standard deviation $\sigma$ is the standard deviation of the error term $Z_i$. Note that adding a constant $\mu$ to a standard normally distributed random variable $Z$ results in a new normally distributed random variable, the mean of which is simply shifted: $Y = \mu + Z$,  $Z \sim \mathcal{N}(0,\sigma)$ $\implies$ $Y \sim \mathcal{N}(\mu,\sigma)$.


So, the likelihood for a single observation $y_i$, given input $x_i$ and parameters $\boldsymbol{\theta}$, is given by

$$
L(\boldsymbol{\theta}) = p(y_i|x_i,\boldsymbol{\theta}) = \frac{1}{\sqrt{2 \pi \sigma^2 }} \exp\left( {-\frac{(y_i - y_{i,det})^2}{2\sigma^2}}  \right)
\;.
$$


Under the assumption of independence\textbf{*}, the likelihood of all observations $\mathbf{y} = \{y_1,y_2,...,y_n\}$, given all inputs $\mathbf{x} = \{x_1,x_2,...,x_n\}$, is then given by 

$$
p(\mathbf{y}|\mathbf{x},\boldsymbol{\theta}) = p(y_1|x_1,\boldsymbol{\theta}) \, p(y_2|x_2,\boldsymbol{\theta}) \, p(y_3|x_3,\boldsymbol{\theta}) \, \dots \, p(y_n|x_n,\boldsymbol{\theta}) = \prod_{i=1}^n p(y_i|x_i,\boldsymbol{\theta})
\;,
$$


which is then given in full by

$$
L(\boldsymbol{\theta}) = p(\mathbf{y}|\mathbf{x},\boldsymbol{\theta}) 
= \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2} } \exp\left( {-\frac{(y_i - y_{i,det})^2}{2\sigma^2}}  \right) \\
= \frac{1}{(2 \pi)^{n/2} \sigma^n } \exp\left( {-\frac{1}{2\sigma^2}} \sum_{i=1}^n (y_i - \beta x_i - \gamma)^2 \right)
\;.
$$





**Note**: If the observations were not independent, the likelihood would instead be given by

$$
p(\mathbf{y}|\mathbf{x},\boldsymbol{\theta}) = p(y_1|\mathbf{x},\boldsymbol{\theta}) \, p(y_2|y_1,\mathbf{x},\boldsymbol{\theta}) \, p(y_3|y_1,y_2,\mathbf{x},\boldsymbol{\theta}) \, \dots \, p(y_n|y_1,y_2,...,y_{n-1},\mathbf{x},\boldsymbol{\theta})
\;.
$$


Only if the observations are independent, this reduces to the simple product given above.





## Likelihood evaluation

The likelihood can be implemented like this:

```{r eval=FALSE}

loglikelihood.linear <- function(x,y,par){
  
  # deterministic part:
  y.det <- par[1] * x + par[2]
  
  # Calculate log-likelihood:
  return( sum( dnorm(y, mean=y.det, sd=par[3], log=TRUE ) ) )
  
}

```


You can then compute the likelihood of some parameters, given the data, for instance like this:

```{r eval=FALSE}

# parameters for testing:
par.linear.test = c(2,1,1)


# compute likelihood for these parameters:
loglikelihood.linear(x,y,par.linear.test)
```

Computing the log-likelihood for the given experimental data and the sets of model parameters, we see that the likelihood of the parameter set $\{\beta = 2,\, \gamma = 1, \, \sigma = 1\}$ is larger. This implies that these parameters are more likely to have generated the data, assuming that the linear model is an accurate description of the data generating mechanism.




## Likelihood optimisation

Calling the `optim` function then looks like this:

```{r  eval=FALSE}

# define starting parameters for optimisation:
beta_init = 2
gamma_init = 1
sigma_init = 1

# find maximising parameters using optim:
maximisingParameters <- optim(par = c(beta_init,gamma_init,sigma_init), fn = loglikelihood.linear, x=x, y=y)

# inspect the result:
maximisingParameters$par
```

**However**: `optim` does minimisation per default, but we want to maximise the likelihood! Therefore, we need to adjust our log-likelihood function to return the negative log-likelihood for this exercise:

```{r  eval=FALSE}

# Calculate the negative log-likelihood:
return( -sum( dnorm(y, mean=y.det, sd=par[3], log=TRUE ) ) )

```

We can check that the result looks reasonable by plotting the resulting linear model together with the data:


```{r  eval=FALSE}

# model output from x-input and estimated parameters:
y.estimate = maximisingParameters$par[1]*ext.data[,'x'] + maximisingParameters$par[2]

# plot the result:
x11()
plot(x,y,xlab='x',ylab='y')
lines(x,y.estimate)

```


![*Figure: Original data (dots) plus linear fit (line).*](./figures/linearFit.png)


Looks ok!



## Linear regression


The R linear regression function `lm` works differently (it uses a method called "QR decomposition"), but should give very similar results to our likelihood optimisation. 





# Likelihood for model "Monod" (recommended) {#task2}


## Analytical expression

For the model "Monod" with additive normal i.i.d. noise, constructing the likelihood follows exactly the same procedure as in the linear case (Exercise 1.1), only with a simple change in the deterministic part of the model. So again, our complete probabilistic model is given by the deterministic part plus a Gaussian noise term,

$$
Y_i = y_{i,det} + Z_i  , \quad \quad Z_i \sim \mathcal{N}(0,\sigma)
\;,
$$

but now the deterministic part is given by

$$
y_{det}(x,\boldsymbol{\theta}_{monod}) = 
r(C,r_{max}, K) =  \frac{r_{max} C}{K+C} 
\;.
\label{eqn:monodDet}
$$

We therefore again have three model parameters in total: $\boldsymbol{\theta} = \{ \boldsymbol{\theta}_{monod},\sigma\} = \{r_{max}, K,\sigma\}$.

The expression for the likelihood is then similar to that of the linear model:

$$
L(\boldsymbol{\theta}) = p(\mathbf{y}|\mathbf{C},r_{max}, K,\sigma) = \frac{1}{(2 \pi)^{n/2} \sigma^n } \exp\left( {-\frac{1}{2\sigma^2}} \sum_{i=1}^n (y_i - r(C_i,r_{max},K))^2 \right)
\;.
$$



## Likelihood evaluation

The implementation then looks like this:


```{r  eval=FALSE}


loglikelihood.monod <- function( y, C, par){
  
  # deterministic part:
  
  y.det <- ( par['r_max'] * C ) / ( par['K'] + C )
  
  # Calculate loglikelihood:
  
  return( sum( dnorm(y, mean=y.det, sd=par['sigma'], log=TRUE ) ) )
}

```

Again, the parameters with higher likelihood are more likely to have generated the data. In our case, this should be the parameter set $\{r_{\mathrm{max}} = 5,\,K = 3,\,\sigma=0.2\}$.

Plotting the data together with the model output given those parameters looks ok too:

![*Figure: External data plus model output for model "Monod"*](./figures/monodFit.png)


# Forward model simulation (recommended)

## Deterministic model simulation

Here is an example of how to run simulations with the model "Monod":

```{r  eval=FALSE}

# variables for model "Monod":
# ----------------------------

C.monod    <- 0:100/10         # concentrations at which model output should be calculated
par.monod  <- c(r_max=5,K=3)   # define parameter vector for the model "Monod"

L.monod <- data.frame(var="r",C=C.monod)  # observation layout 
```

The observation layout is of course not universal, but required in this form for the `model.monod` function implemented in `EawagSchoolTools`, which can be called like this:


```{r  eval=FALSE}

# simulation with model "monod":
# ------------------------------

res.monod <- model.monod(par.monod,L.monod)                 # call model
print(res.monod)                                            # print results
x11()                                                       # open plot window
Plot.res(res.monod,L.monod,header="Model: monod",xlab="C")  # plot results
```

For the simulations with model "Growth", see the [R-script](./exercise_1_likelihood_sensitivity.R).

The model outputs should look something like this:

![*Figure: Deterministic model output: model "Monod"*](./figures/deterministicMonod.png)

![*Figure: Deterministic model output: model "Growth"*](./figures/deterministicGrowth.png)


## Stochastic model simulation


Suppose that we want to produce stochastic model outputs for model parameters $\boldsymbol{\theta} = \{r_{\mathrm{max}},K,\sigma\}$ and input variables  $\mathbf{C} = (C_1,...,C_{n})$. First, we calculate the deterministic growth rate, for each concentration in $\mathbf{C}$, and store the results in vector $\mathbf{y}_{\mathrm{det}} = \mathbf{r}(\mathbf{C},r_{\mathrm{max}},K) = \{r(C_1,r_{\mathrm{max}},K),\dots,r(C_n,r_{\mathrm{max}},K)\}$.
Next, we simulate the measurement errors by generating $n$ random normal values with mean $0$ and standard deviation $\sigma$ and store the results in vector $\mathbf{z}$. Both vectors can then be added to get the probabilistic model output.

See the [R-script](./exercise_1_likelihood_sensitivity.R) for the implementation.

The model output will look something like this, where the simulated quantiles 2.5%, 50% and 97.5% are indicated:


![*Figure: Probabilistic model output: model "Monod"*](./figures/probabilisticMonod.png)




# Likelihood and forward simulation for the model "Survival"

## Analytical expression for the likelihood

The probability for an individual with mortality rate $\lambda$ to survive until time $t$ is given by the exponential
$$
S(t,\lambda)=e^{-\lambda t}\,.
$$

Thus, the probability of the individual to die within time interval $[t_{i-1},t_i]$ is given by
$$
p_{i,\lambda}=S(t_{i-1},\lambda) - S(t_i,\lambda)\,.
$$

Assume that we have $N$ individuals, each with the same mortality rate.
We want to calculate the probability that $y_1$ individuals die in time interval $[t_0,t_1]$, $y_2$ individuals die in time interval $[t_2,t_3]$ etc.
Those individuals who survive the whole experiment die in the time interval $[t_n,\infty]$, and this happens with probability $p_{n+1}=S(t_n)$.
For the time being, assume that we distinguish the individuals and want to calculate the probability that individual 1 dies in, say, time interval $[t_3,t_4]$, individual 2 in time interval, say, $[t_1,t_2]$ etc. in such a way that the counts $\bf y$ are assumed.
Due to the independence of the individuals, the probability, for such an event, is then simply given by the product
$$
p_{1,\lambda}^{y_1} \cdot p_{2,\lambda}^{y_2} \cdot \ldots \cdot \, p_{n,\lambda}^{y_n}\,.
$$

Since we do not distinguish individuals, we have to multiply this product by a so-called multinomial coefficient, which counts the number of ways to put $N$ individuals into $n+1$ buckets, with $y_1$ ending up in the first bucket etc.
Thus, the likelihood function, for model parameter $\lambda$, given an output $\bf y$ of our survival model is described by the so-called multinomial distribution
$$
L_{\texttt{survival}}(\lambda,{\bf y})
=
\frac{N!}{y_1!\cdots y_{n+1}!} p_{1,\lambda}^{y_1} \cdot p_{2,\lambda}^{y_2} \cdot \ldots \cdot p_{n+1,\lambda}^{y_{n+1}}\,.
$$

## Forward simulation

The simplest way to simulate a model output is to use the R function `rmultinom`.
Alternatively, we can simulate the death time, for each individual, drawing from the exponential distribution
$$
f(t|\lambda)=\lambda e^{-\lambda t}\,,
$$
which is the negative derivative of the survival probability $S(t)$.
After that, we simply have to count how many individuals die within the different time intervals.
See the [R-script](./exercise_1_likelihood_sensitivity.R) for the implementation.

Here we show the quantiles of the model output, for fixed parameters, in the form of a boxplot:


![*Figure: Model output: "Survival"*](./figures/modelSurvival.png)



## Likelihood evaluation

See the [R-script](./exercise_1_likelihood_sensitivity.R) for the implementation.
The likelihood should be highest for the parameter that you used to generate the data.



# Sensitivity analysis


## Manual sensitivity analysis

Simulations with one parameter incremented by 10\% (top) and one parameter incremented by 50\% (bottom) are shown in the plots below. The incremented parameter is indicated in the header of the plot. These are the results for the model "Monod":

![*Figure: Manual sensitivity analysis model "Monod": 10%.*](./figures/model_monod_sens_man_0.1.png)

![*Figure: Manual sensitivity analysis model "Monod":50%.*](./figures/model_monod_sens_man_0.5.png)



For the model "Growth", it looks like this:

![*Figure: Manual sensitivity analysis model "Growth": 10%.*](./figures/model_growth_sens_man_0.1.png)
![*Figure: Manual sensitivity analysis model "Growth": 50%.*](./figures/model_growth_sens_man_0.5.png)




## Local sensitivity analysis

Local sensitivity functions according to Equation (7.3) with $\Delta \theta_{M,j} = 0.1 \theta_{M,j}$ (top) and $\Delta \theta_{M,j} = 0.5 \theta_{M,j}$ (bottom) are shown in the figures below.
The incremented parameter is indicated in the header of the plot.
In local sensitivity analysis, the shape is independent of  $\Delta \theta_{M,j}$ so that the functions in the right panels are just a multiple of the functions in the left panels.


![*Figure: Local sensitivity analysis model "Monod": factor 0.1.*](./figures/model_monod_sens_reg2_0.1.png)
![*Figure: Local sensitivity analysis model "Monod": factor 0.5.*](./figures/model_monod_sens_man_0.5.png)


For the model "Growth", it looks like this:

![*Figure: Local sensitivity analysis model "Growth": factor 0.1.*](./figures/model_growth_sens_reg2_0.1.png)
![*Figure: Local sensitivity analysis model "Growth": factor 0.5.*](./figures/model_growth_sens_man_0.5.png)



## Variance-based sensitivity

Variance-based regional sensitivity functions according to Equation (7.19) with independent lognormal marginals with standard deviations $\operatorname{SD}\bigl[\Theta_{M,j}\bigr] = 0.1 \theta_{M,j}$ (top) and $\operatorname{SD}\bigl[\Theta_{M,j}\bigr] = 0.5 \theta_{M,j}$ (bottom) are shown in the figures below.
The incremented parameter is indicated in the header of the plot.

![*Figure: Regional sensitivity analysis model "Monod": factor 0.1.*](./figures/model_monod_sens_reg2_0.1.png)
![*Figure: Regional sensitivity analysis model "Monod": factor 0.5.*](./figures/model_monod_sens_reg2_0.5.png)



For the model "Growth", it looks like this:


![*Figure: Regional sensitivity analysis model "Growth": factor 0.1.*](./figures/model_growth_sens_reg2_0.1.png)

![*Figure: Regional sensitivity analysis model "Growth": factor 0.5.*](./figures/model_growth_sens_reg2_0.5.png)

## Comparing different sensitivity measures

Variance-based sensitivity functions do not reflect the sign of the change (see Equation (7.19)), but their shape would be the same as that of the linear sensitivity functions if the model equations were linear in the parameters.
As this is more difficult to fulfill with wider parameter distributions, the plots in the left panels in the regional sensitivity are better approximations to the local analyses.
But nonlinear effects are also not strong for the wider distribution (bottom panels).


