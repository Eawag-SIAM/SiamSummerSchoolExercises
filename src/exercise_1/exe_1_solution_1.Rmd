## Solutions

### Analytical expression

For a one-dimensional linear model with an additive error, a model output  $Y$ is given by the deterministic term $y_{det}$ plus the noise term. For the noise term we choose here identical independent normal errors, so that for a single observation $i$ (in a set of $n$ observations) the full model is given by

$$
Y_i = y_{i,det} + Z_i  , \quad \quad Z_i \sim \mathcal{N}(0,\sigma) , \quad \quad i \in \{1,\dots,n\}
\;.
$$


For the deterministic part, we have an input $x$ and two model parameters $\{\beta, \gamma\}$, from which $y_{det}$ is given by

$$
y_{det}(x,\beta, \gamma) =  \beta  x + \gamma
\;.
$$


We therefore have three model parameters in total: $\beta$ and $\gamma$ from the deterministic term, and $\sigma$ from the noise term. For convenience, we collect these into the set of parameters ${\boldsymbol{\theta}} = \{\beta,\gamma,\sigma\}$.


So for a single observation, the model is given by

$$
Y_i = \beta  x_i + \gamma +Z_i
\;,
$$


which is equivalent to

$$
Y_i \sim \mathcal{N}\left(y_{i,det}(x_i,\beta,\gamma),\sigma\right)
\;.
$$


That is, the model output $Y_i$ is normally distributed, where the mean is the deterministic model output $y_{i,det}$ and the standard deviation $\sigma$ is the standard deviation of the error term $Z_i$. Note that adding a constant $\mu$ to a standard normally distributed random variable $Z$ results in a new normally distributed random variable, the mean of which is simply shifted: $Y = \mu + Z$,  $Z \sim \mathcal{N}(0,\sigma)$ $\implies$ $Y \sim \mathcal{N}(\mu,\sigma)$.


So, the likelihood for a single observation $y_i$, given input $x_i$ and parameters $\boldsymbol{\theta}$, is given by

$$
p(y_i|x_i,\boldsymbol{\theta}) = \frac{1}{\sqrt{2 \pi \sigma^2 }} \exp\left( {-\frac{(y_i - y_{i,det})^2}{2\sigma^2}}  \right)
\;.
$$


Thanks to the assumption of independence, the likelihood of all observations $\mathbf{y} = \{y_1,y_2,...,y_n\}$, given all inputs $\mathbf{x} = \{x_1,x_2,...,x_n\}$, is then given by

$$
p(\mathbf{y}|\mathbf{x},\boldsymbol{\theta}) = p(y_1|x_1,\boldsymbol{\theta}) \, p(y_2|x_2,\boldsymbol{\theta}) \, p(y_3|x_3,\boldsymbol{\theta}) \, \dots \, p(y_n|x_n,\boldsymbol{\theta}) = \prod_{i=1}^n p(y_i|x_i,\boldsymbol{\theta})
\;,
$$


which is then given in full by

$$
p(\mathbf{y}|\mathbf{x},\boldsymbol{\theta}) = L(\boldsymbol{\theta})
= \prod_{i=1}^n p(y_i|x_i,\boldsymbol{\theta})
= \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2} } \exp\left( {-\frac{(y_i - y_{i,det})^2}{2\sigma^2}}  \right) \;.
$$



### Likelihood evaluation {.tabset}

#### R

The likelihood can be implemented like this:

```{r}

loglikelihood.linear <- function(x, y, par){
    ## deterministic part:
    y.det <- par[1] * x + par[2]

    ## Calculate log-likelihood:
    return( sum(dnorm(y, mean=y.det, sd=par[3], log=TRUE)) )
}

```


You can then compute the likelihood of some parameters, given the data, for instance like this:

```{r}
## read data
dat <- read.table("../../data/model_linear.csv", header=TRUE)
x <- dat$x
y <- dat$y

## parameters for testing:
par.linear.test = c(2, 1, 1)

## compute likelihood for these parameters:
loglikelihood.linear(x, y, par.linear.test)
```

Computing the log-likelihood for the given experimental data and the sets of model parameters, we see that the likelihood of the parameter set $\{\beta = 2,\, \gamma = 1, \, \sigma = 1\}$ is larger. This implies that these parameters are more likely to have generated the data, assuming that the linear model is an accurate description of the data generating mechanism.

#### Julia
The likelihood can be implemented like this
```{julia, results = 'hide'}
using Distributions

function loglikelihood_linear(beta, gamma, sigma, x, y)
    y_det = beta .* x .+ gamma
    return sum(logpdf.(Normal.(y_det, sigma), y))
end
```
Then read you data so that you can manipulate it
```{julia, results = 'hide'}
using DataFrames
import CSV

linear_data = CSV.read(joinpath("..", "..", "data", "model_linear.csv"),
                       DataFrame)
#show(linear_data, allrows=true) if you want to see all your data

x = linear_data.x
y = linear_data.y
```
Compute
```{julia}
#Log likelihood with parameters β = 2, γ = 1, σ = 1
loglikelihood_linear(2,1,1, x, y)
#Log likelihood with parameters β = 3, γ = 2, σ = 1
loglikelihood_linear(3,2,1, x, y)
#parameters  β = 2, γ = 1, σ = 1 are more likely to have produced to data
```

#### Python

The following libraries and modules are required:
```{python, results = 'hide'}
from models import model_monod, model_growth
import pandas as pd
import numpy as np
from scipy.stats import norm, multinomial
import matplotlib.pyplot as plt
import seaborn as sns
```
The likelihood can be implemented like this:

```{python}
def loglikelihood_linear(x, y, par):
    """
    Calculate the log-likelihood for a linear model.

    Parameters:
    x (array-like): Independent variable.
    y (array-like): Dependent variable.
    par (list or array-like): Parameters of the model [slope, intercept, standard deviation].

    Returns:
    float: The log-likelihood value.
    """
    # Deterministic part
    y_det = par[0] * x + par[1]
    
    # Calculate log-likelihood
    log_likelihood = np.sum(norm.logpdf(y, loc=y_det, scale=par[2]))
    
    return log_likelihood
```


You can then compute the likelihood of some parameters, given the data, for instance like this:

```{python}
# Loading data
# Specify the path to the CSV file
file_path = r"../../data/model_linear.csv"

# Load the CSV file into a pandas DataFrame
dat = pd.read_csv(file_path, sep=" ")

# Extract x and y columns
x = dat['x']
y = dat['y']

# Parameters for testing
par_linear_test = [2, 1, 1]

# Compute likelihood for these parameters
result = loglikelihood_linear(x, y, par_linear_test)
print(f"Log-likelihood: {result}")
```

### Likelihood optimisation {.tabset}

#### R

Calling the `optim` function then looks like this:

```{r}
## define starting parameters for optimisation:
par.init <- c(beta = 2,
              gamma = 1,
              sigma = 1)

## because `optim` is minimizing, we define the negative loglikelihood
neg.loglikelihood.linear <- function(x, y, par) -loglikelihood.linear(x, y, par)

## maximum likelihood estimation
MLEparameters <- optim(par = par.init, fn = neg.loglikelihood.linear, x=x, y=y)

## inspect the result:
MLEparameters$par
```


We can check that the result looks reasonable by plotting the resulting linear model together with the data:


```{r}

# model output from x-input and estimated parameters:
y.estimate = MLEparameters$par[1]*x + MLEparameters$par[2]

plot(x, y)
lines(x, y.estimate, col=2)

```

#### Julia
Implementing the negative loglikelihood that we will minimize. Note,
we optimize for the `log_sigma` to avoid any negative values for `sigma`.
```{julia, results = 'hide'}
function neg_loglikelihood_linear(θ::Vector)
    beta, gamma, log_sigma = θ
    # minus sign to be able to minimize after
    - loglikelihood_linear(beta, gamma, exp(log_sigma), x, y)
end
```
Minimizing
```{julia}
using Optim
θinit = [2.0, 1, log(0.5)]
param = Optim.minimizer(optimize(neg_loglikelihood_linear, θinit));

param[3] = exp(param[3])        # back-transform to sigma
param
```


Plotting
```{julia, out.width = '80%'}
using Plots

scatter(x, y,
    labels=false,
    xlabel = "x",
    ylabel = "y");
plot!(x -> param[1]*x + param[2],
      labels = false)
```

#### Python

Calling the `optim` function then looks like this:

```{python}
from scipy.optimize import minimize

# Define the negative log-likelihood function
def neg_loglikelihood_linear(par, x, y):
    return -loglikelihood_linear(x, y, par)

# Define starting parameters for optimization
par_init = [2, 1, 1]  # [beta, gamma, sigma]

# Perform maximum likelihood estimation
result = minimize(neg_loglikelihood_linear, par_init, args=(x, y))

# Inspect the result
MLEparameters = result.x
print(f"MLE Parameters: {MLEparameters}")
```


We can check that the result looks reasonable by plotting the resulting linear model together with the data:


```{python}
# Model output from x-input and estimated parameters
y_estimate = MLEparameters[0] * x + MLEparameters[1]

# Plot the original data
plt.figure(figsize=(14, 6))

plt.scatter(x, y, label='Observed Data')

# Plot the estimated values
plt.plot(x, y_estimate, color='red', label='Estimated Model')

# Add labels and legend
plt.xlabel('x')
plt.ylabel('y')
plt.legend()

# Adjust layout and display the plots
plt.tight_layout()
plt.show()
```

### Linear regression {.tabset}

The linear regression functions  works differently (it uses a
method called "QR decomposition"), but should give very similar
results to our likelihood optimisation.

#### R

```{r}
mod <- lm(y~x)
mod
```
Note, `(intercept)` corresponds to $\gamma$ and `x` to $\beta$.


#### Julia
```{julia}
using GLM
linear_mod = GLM.lm(@formula(y ~ x), linear_data)
```
Note, `(intercept)` corresponds to $\gamma$ and `x` to $\beta$.

#### Python

```{python}
import statsmodels.api as sm

# Add a constant term (intercept) to the independent variable
x_with_const = sm.add_constant(x)

# Fit the linear model
model = sm.OLS(y, x_with_const).fit()

# Print the summary of the model
print(model.summary())
```
Note, `(intercept)` corresponds to $\gamma$ and `x` to $\beta$.
