# 2. Solution {.tabset}

## R

### Read data

```{r}
data.monod <- read.table("../../data/model_monod_stoch.csv", header=T)
plot(data.monod$C, data.monod$r, xlab="C", ylab="r", main='"model_monod_stoch.csv"')
```

### Define likelihood, prior, and posterior
```{r}
source("../../models/models.r") # load the monod model

## Logprior for model "monod": lognormal distribution

prior.monod.mean <- 0.5 * c(r_max=5, K=3, sigma=0.5)
prior.monod.sd   <- 0.5 * prior.monod.mean

logprior.monod <- function(par, mean, sd){
    sdlog <- sqrt(log(1+sd*sd/(mean*mean)))
    meanlog <- log(mean) - sdlog*sdlog/2
    return(sum(dlnorm(par, meanlog=meanlog, sdlog=sdlog, log=TRUE)))
}

## Log-likelihood for model "monod"

loglikelihood.monod <- function(y, C, par){

    ## deterministic part:
    y.det <- model.monod(par, C) # defined in `models.r`

    ## Calculate loglikelihood assuming independence:
    return( sum(dnorm(y, mean=y.det, sd=par['sigma'], log=TRUE )) )
}

## Log-posterior for model "monod"

logposterior.monod <- function(par) {
    lp <- logprior.monod(par, prior.monod.mean, prior.monod.sd)
    if(is.finite(lp)){
        return( lp + loglikelihood.monod(data.monod$y, L.monod$y, par) )
    } else {
        return(-Inf)
    }
}
```

### Create initial chain

### Improve jump distribution

### Residual Diagnostics




## Julia

### Read data

```{julia}
using DataFrames
import CSV
using Plots

monod_data = CSV.read("../../data/model_monod_stoch.csv", DataFrame)

scatter(monod_data.C, monod_data.r,
        label=false, xlab="C", ylab="r")
```

### Define likelihood, prior, and posterior

```{julia}
include("../../models/models.jl"); # load the definition of `model_monod`
using ComponentArrays

# set parameters
par = ComponentVector(r_max = 5, K=3, sigma=0.5);

prior_monod_mean = 0.5 .* par;
prior_monod_sd = 0.25 .* par;

# Use a lognormal distribution for all model parameters
function logprior_monod(par, m, sd)
    μ = @. log(m/sqrt(1+sd^2/m^2))
    σ = @. sqrt(log(1+sd^2/m^2))
    return sum(logpdf.(LogNormal.(μ, σ), par)) # sum because we are in the log-space
end

# Log-likelihood for model "monod"
function loglikelihood_monod(par::ComponentVector, data::DataFrame)
    y_det = model_monod(data.C, par)
    return sum(logpdf.(Normal.(y_det, par.sigma), data.r))
end

# Log-posterior for model "monod"
function logposterior_monod(par::ComponentVector)
    lp = logprior_monod(par, prior_monod_mean, prior_monod_sd)
    if !isinf(lp)
        lp += loglikelihood_monod(par, monod_data)
    end
    lp
end
```

### Create initial chain

```{julia, result="hide", echo=FALSE}
using LinearAlgebra
using Distributions

function metropolis_MCMC(lp::Function, θinit; nsamples = 1000,
                         Σjump = diagm(ones(length(θinit))))

    dims = length(θinit)        # get number of parameters

    chain = Vector{typeof(par)}(undef, nsamples)
    log_posts = Vector{Float64}(undef, nsamples)

    # compute logposterior value of inits parameters
    chain[1] = θinit
    log_posts[1] = lp(θinit)

    # Run the MCMC
    for i in 2:nsamples
        # Propose new parameter value
        θprop = chain[i-1] .+ rand(MvNormal(zeros(dims), Σjump))

        # Calculate logposterior of the proposed parameter value
        logpost_prop = lp(θprop)

        # Calculate acceptance probability
        acc_prob = min(1, exp(logpost_prop - log_posts[i-1]))

        # Store in chain[i] the new parameter value if accepted, or re-store old one if rejected:
        if rand() < acc_prob #
            chain[i] = θprop
            log_posts[i] = logpost_prop
        else # if not accepted, stay at the current parameter value
            chain[i] = chain[i-1]
            log_posts[i] = log_posts[i-1]
        end
    end

    (chain = chain, lp = log_posts)
end
```

We use the little function `metropolis_MCMC` that we have defined above.

```{julia}
using MCMCChains
using StatsPlots

Σ = diagm(ones(3))          # covariance of uncorrelated jump distribution
res = metropolis_MCMC(logposterior_monod, par; Σjump = Σ, nsamples=5000);

# We convert the sampeles in a `MCMCChains.Chains` object to
# get plotting and summary functions

chn = Chains(res.chain, labels(par))

plot(chn)
corner(chn)
```

### Improve jump distribution

The corner plot showed that the parameters $k$ and $r_{max}$ are
clearly correlated and the chain plots that the jump distribution is
too large.

We use the previous sample to estimate the covariance matrix which is
then used as new jump distribution.

```{julia}
# take covariance for previous chain
Σ2 = cov(Array(chn)) * (1/2)^2

res2 = metropolis_MCMC(logposterior_monod, par;
                       nsamples=5000,
                               Σjump = Σ2);

chn2 = Chains(res2.chain, labels(par))

plot(chn2)
corner(chn2)
```

Much better!

### Residual Diagnostics

First we compute the residuals with the 'best' parameters
```{julia, results=FALSE}
# get maximum of posterior parameters
par_max = res2.chain[argmax(res2.lp)]

# compute residuals
residuals = monod_data.r .- model_monod(monod_data.C, par_max)
```

Plotting the residuals. We hope not to find any structure, but this
model is clearly not perfect:
```{julia}
p = scatter(monod_data.r, residuals, label=false, xlab="r", ylab="residual");
hline!(p, [0], label=false)
```

Quantile-quantile plots are great to see if the residuals are
normal distributed (which was out assumption for the likelihood
functions). Here we see that we have too heavy tails.
```{julia}
qqnorm(residuals)               # a bit heavy-tailed
```

We also assumed independence of the observations, i.e. we should not
see any auto-correlation. This looks good here:
```{julia}
using StatsBase
acf = StatsBase.autocor(residuals);
bar(0:(length(acf)-1), acf, label=false, xlab="lag", ylab="correlation")
```
