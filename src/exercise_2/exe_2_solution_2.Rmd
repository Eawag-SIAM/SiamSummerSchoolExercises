# 2. Solution {.tabset}

## R

### Read data

```{r}
data.monod <- read.table("../../data/model_monod_stoch.csv", header=T)
plot(data.monod$C, data.monod$r, xlab="C", ylab="r", main='"model_monod_stoch.csv"')
```

### Define likelihood, prior, and posterior
```{r}
source("../../models/models.r") # load the monod model

## Logprior for model "monod": lognormal distribution

prior.monod.mean <- 0.5 * c(r_max=5, K=3, sigma=0.5)
prior.monod.sd   <- 0.5 * prior.monod.mean

logprior.monod <- function(par, mean, sd){
    sdlog <- sqrt(log(1+sd*sd/(mean*mean)))
    meanlog <- log(mean) - sdlog*sdlog/2
    return(sum(dlnorm(par, meanlog=meanlog, sdlog=sdlog, log=TRUE)))
}

## Log-likelihood for model "monod"

loglikelihood.monod <- function(y, C, par){

    ## deterministic part:
    y.det <- model.monod(par, C) # defined in `models.r`

    ## Calculate loglikelihood assuming independence:
    return( sum(dnorm(y, mean=y.det, sd=par['sigma'], log=TRUE )) )
}

## Log-posterior for model "monod"

logposterior.monod <- function(par) {
    lp <- logprior.monod(par, prior.monod.mean, prior.monod.sd)
    if(is.finite(lp)){
        return( lp + loglikelihood.monod(data.monod$r, data.monod$C, par) )
    } else {
        return(-Inf)
    }
}
```

### Create initial chain

```{r}
library(adaptMCMC)

## As start values for the Markov chain we can use the mean of the prior
par.init <- c(r_max=2.5, K=1.5, sigma=0.25)

## sample
monod.chain <- adaptMCMC::MCMC(p = logposterior.monod,
                               n = 5000,
                               init = par.init,
                               scale = c(1,1,1),
                               adapt = FALSE, # for this exercise we do not
                                              # want to use automatic adaptation
                               showProgressBar = FALSE
                               )
monod.chain.coda <- adaptMCMC::convert.to.coda(monod.chain) # this is useful for plotting
summary(monod.chain.coda)
```

The acceptance rate is very low:
```{r}
monod.chain$acceptance.rate
```

Them poor mixing is also visible from the chain plots:
```{r}
plot(monod.chain.coda)
```

Here we see a strong correlation between $K$ and $r_{max}$:
```{r}
pairs(monod.chain$samples)
```

### Improve jump distribution

We use the first chain and estimates it's covariance matrix which we
can use as jump distribution in a second run:
```{r}
Sigma.jump <- cov(monod.chain$samples) * (1/2)^2

monod.chain2 <- adaptMCMC::MCMC(p = logposterior.monod,
                                n = 5000,
                                init = par.init,
                                scale = Sigma.jump,
                                adapt = FALSE,
                                showProgressBar = FALSE
                                )
monod.chain.coda2 <- adaptMCMC::convert.to.coda(monod.chain2)
summary(monod.chain.coda2)
```

Everything looks much better now!
```{r}
monod.chain2$acceptance.rate
```

```{r}
plot(monod.chain.coda2)
```

```{r}
pairs(monod.chain2$samples)
```

### Residual Diagnostics


First we compute the residuals with the 'best' parameters
```{r, results=FALSE}
# get maximum of posterior parameters
par.max <- monod.chain2$samples[which.max(monod.chain2$log.p),]

residuals <- data.monod$r - model.monod(par.max, C=data.monod$C)
```

Plotting the residuals. We hope not to find any structure, but this
model is clearly not perfect:
```{r}
plot(data.monod$r, residuals, xlab="r")
abline(h=0, col=2)
```

Quantile-quantile plots are great to see if the residuals are
normal distributed (which was out assumption for the likelihood
functions). Here we see that we have too heavy tails.
```{r}
qqnorm(residuals)
qqline(residuals)
```

We also assumed independence of the observations, i.e. we should not
see any auto-correlation. This looks good here:
```{r}
acf(residuals)
```



## Julia

### Read data

```{julia}
using DataFrames
import CSV
using Plots

monod_data = CSV.read("../../data/model_monod_stoch.csv", DataFrame)

scatter(monod_data.C, monod_data.r,
        label=false, xlab="C", ylab="r")
```

### Define likelihood, prior, and posterior

```{julia}
include("../../models/models.jl"); # load the definition of `model_monod`
using ComponentArrays

# set parameters
par = ComponentVector(r_max = 5, K=3, sigma=0.5);

prior_monod_mean = 0.5 .* par;
prior_monod_sd = 0.25 .* par;

# Use a lognormal distribution for all model parameters
function logprior_monod(par, m, sd)
    μ = @. log(m/sqrt(1+sd^2/m^2))
    σ = @. sqrt(log(1+sd^2/m^2))
    return sum(logpdf.(LogNormal.(μ, σ), par)) # sum because we are in the log-space
end

# Log-likelihood for model "monod"
function loglikelihood_monod(par::ComponentVector, data::DataFrame)
    y_det = model_monod(data.C, par)
    return sum(logpdf.(Normal.(y_det, par.sigma), data.r))
end

# Log-posterior for model "monod"
function logposterior_monod(par::ComponentVector)
    lp = logprior_monod(par, prior_monod_mean, prior_monod_sd)
    if !isinf(lp)
        lp += loglikelihood_monod(par, monod_data)
    end
    lp
end
```

### Create initial chain

We use the [`KissMCMC`](https://github.com/mauro3/KissMCMC.jl) package
which provides a very basic metropolis sampler.

```{julia}
using KissMCMC: metropolis
using Distributions
using LinearAlgebra: diagm
using MCMCChains

# define a function that generates a proposal given θ:
Σjump = diagm(ones(3))
sample_proposal(θ) = θ .+ rand(MvNormal(zeros(length(θ)), Σjump))

# run sampler
samples, acceptance_ratio, lp = metropolis(logposterior_monod,
                                           sample_proposal,
                                           par;
                                           niter = 5_000,
                                           nburnin = 0);


# We convert the sampeles in a `MCMCChains.Chains` object to
# get plotting and summary functions

chn = Chains(samples, labels(par))

plot(chn)
corner(chn)
```

### Improve jump distribution

The corner plot showed that the parameters $k$ and $r_{max}$ are
clearly correlated and the chain plots that the jump distribution is
too large.

We use the previous sample to estimate the covariance matrix which is
then used as new jump distribution.

```{julia}
# take covariance for previous chain
Σjump = cov(Array(chn)) * (1/2)^2
sample_proposal(θ) = θ .+ rand(MvNormal(zeros(length(θ)), Σjump))

samples2, acceptance_ratio2, lp2 = metropolis(logposterior_monod,
                                              sample_proposal,
                                              par;
                                              niter = 5_000,
                                              nburnin = 0);

chn2 = Chains(samples2, labels(par))

plot(chn2)
corner(chn2)
```

Much better!

### Residual Diagnostics

First we compute the residuals with the 'best' parameters
```{julia, results=FALSE}
# get maximum of posterior parameters
par_max = samples2[argmax(lp2)]

# compute residuals
residuals = monod_data.r .- model_monod(monod_data.C, par_max)
```

Plotting the residuals. We hope not to find any structure, but this
model is clearly not perfect:
```{julia}
p = scatter(monod_data.r, residuals, label=false, xlab="r", ylab="residual");
hline!(p, [0], label=false)
```

Quantile-quantile plots are great to see if the residuals are
normal distributed (which was out assumption for the likelihood
functions). Here we see that we have too heavy tails.
```{julia}
qqnorm(residuals, qqline = :R)               # a bit heavy-tailed
```

We also assumed independence of the observations, i.e. we should not
see any auto-correlation. This looks good here:
```{julia}
using StatsBase
acf = StatsBase.autocor(residuals);
bar(0:(length(acf)-1), acf, label=false, xlab="lag", ylab="correlation")
```
