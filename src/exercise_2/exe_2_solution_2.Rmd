## Solution {.tabset}

### R

#### Read data

```{r}
data.monod <- read.table("../../data/model_monod_stoch.csv", header=T)
plot(data.monod$C, data.monod$r, xlab="C", ylab="r", main='"model_monod_stoch.csv"')
```

#### Define likelihood, prior, and posterior
```{r}
source("../../models/models.r") # load the monod model

## Logprior for model "monod": lognormal distribution

prior.monod.mean <- 0.5 * c(r_max=5, K=3, sigma=0.5)
prior.monod.sd   <- 0.5 * prior.monod.mean

logprior.monod <- function(par, mean, sd){
    sdlog <- sqrt(log(1+sd*sd/(mean*mean)))
    meanlog <- log(mean) - sdlog*sdlog/2
    return(sum(dlnorm(par, meanlog=meanlog, sdlog=sdlog, log=TRUE)))
}

## Log-likelihood for model "monod"

loglikelihood.monod <- function(y, C, par){

    ## deterministic part:
    y.det <- model.monod(par, C) # defined in `models.r`

    ## Calculate loglikelihood assuming independence:
    return( sum(dnorm(y, mean=y.det, sd=par['sigma'], log=TRUE )) )
}

## Log-posterior for model "monod"

logposterior.monod <- function(par) {
    lp <- logprior.monod(par, prior.monod.mean, prior.monod.sd)
    if(is.finite(lp)){
        return( lp + loglikelihood.monod(data.monod$r, data.monod$C, par) )
    } else {
        return(-Inf)
    }
}
```

#### Create initial chain

```{r}
library(adaptMCMC)

## As start values for the Markov chain we can use the mean of the prior
par.init <- c(r_max=2.5, K=1.5, sigma=0.25)

## sample
monod.chain <- adaptMCMC::MCMC(p = logposterior.monod,
                               n = 5000,
                               init = par.init,
                               scale = c(1,1,1),
                               adapt = FALSE, # for this exercise we do not
                                              # want to use automatic adaptation
                               showProgressBar = FALSE
                               )
monod.chain.coda <- adaptMCMC::convert.to.coda(monod.chain) # this is useful for plotting
summary(monod.chain.coda)
```

The acceptance rate is very low:
```{r}
monod.chain$acceptance.rate
```

Them poor mixing is also visible from the chain plots:
```{r}
plot(monod.chain.coda)
```

Here we see a strong correlation between $K$ and $r_{max}$:
```{r}
pairs(monod.chain$samples)
```

#### Improve jump distribution

We use the first chain and estimates it's covariance matrix which we
can use as jump distribution in a second run:
```{r}
Sigma.jump <- cov(monod.chain$samples) * (1/2)^2

monod.chain2 <- adaptMCMC::MCMC(p = logposterior.monod,
                                n = 5000,
                                init = par.init,
                                scale = Sigma.jump,
                                adapt = FALSE,
                                showProgressBar = FALSE
                                )
monod.chain.coda2 <- adaptMCMC::convert.to.coda(monod.chain2)
summary(monod.chain.coda2)
```

Everything looks much better now!
```{r}
monod.chain2$acceptance.rate
```

```{r}
plot(monod.chain.coda2)
```

```{r}
pairs(monod.chain2$samples)
```

#### Residual Diagnostics


First we compute the residuals with the 'best' parameters
```{r, results=FALSE}
# get maximum of posterior parameters
par.max <- monod.chain2$samples[which.max(monod.chain2$log.p),]

residuals <- data.monod$r - model.monod(par.max, C=data.monod$C)
```

Plotting the residuals. We hope not to find any structure, but this
model is clearly not perfect:
```{r}
plot(data.monod$r, residuals, xlab="r")
abline(h=0, col=2)
```

Quantile-quantile plots are great to see if the residuals are
normal distributed (which was out assumption for the likelihood
functions). Here we see that we have too heavy tails.
```{r}
qqnorm(residuals)
qqline(residuals)
```

We also assumed independence of the observations, i.e. we should not
see any auto-correlation. This looks good here:
```{r}
acf(residuals)
```



### Julia

#### Read data

```{julia, out.width = '80%'}
using DataFrames
import CSV
using Plots

monod_data = CSV.read("../../data/model_monod_stoch.csv", DataFrame)
scatter(monod_data.C, monod_data.r,
        label=false, xlab="C", ylab="r")
```

#### Define likelihood, prior, and posterior

```{julia}
include("../../models/models.jl"); # load the definition of `model_monod`
using ComponentArrays

# set parameters
par = ComponentVector(r_max = 5, K=3, sigma=0.5);

prior_monod_mean = 0.5 .* par;
prior_monod_sd = 0.25 .* par;

# Use a lognormal distribution for all model parameters
function logprior_monod(par, m, sd)
    μ = @. log(m/sqrt(1+sd^2/m^2))
    σ = @. sqrt(log(1+sd^2/m^2))
    return sum(logpdf.(LogNormal.(μ, σ), par)) # sum because we are in the log-space
end

# Log-likelihood for model "monod"
function loglikelihood_monod(par::ComponentVector, data::DataFrame)
    y_det = model_monod(data.C, par)
    return sum(logpdf.(Normal.(y_det, par.sigma), data.r))
end

# Log-posterior for model "monod"
function logposterior_monod(par::ComponentVector)
    lp = logprior_monod(par, prior_monod_mean, prior_monod_sd)
    if !isinf(lp)
        lp += loglikelihood_monod(par, monod_data)
    end
    lp
end
```

#### Create initial chain

We use the [`KissMCMC`](https://github.com/mauro3/KissMCMC.jl) package
which provides a very basic metropolis sampler.

```{julia, out.width = '80%'}
using KissMCMC: metropolis
using Distributions
using LinearAlgebra: diagm
using MCMCChains
using StatsPlots

# define a function that generates a proposal given θ:
Σjump = diagm(ones(3))
sample_proposal(θ) = θ .+ rand(MvNormal(zeros(length(θ)), Σjump))
# run sampler
samples, acceptance_ratio, lp = metropolis(logposterior_monod,
                                           sample_proposal,
                                           par;
                                           niter = 5_000,
                                           nburnin = 0);


# We convert the sampeles in a `MCMCChains.Chains` object to
# get plotting and summary functions
chn = Chains(samples, labels(par))
plot(chn)
corner(chn)
```

#### Improve jump distribution

The corner plot showed that the parameters $k$ and $r_{max}$ are
clearly correlated and the chain plots that the jump distribution is
too large.

We use the previous sample to estimate the covariance matrix which is
then used as new jump distribution.

```{julia, out.width = '80%'}
# take covariance for previous chain
Σjump = cov(Array(chn)) * (1/2)^2
sample_proposal(θ) = θ .+ rand(MvNormal(zeros(length(θ)), Σjump))
samples2, acceptance_ratio2, lp2 = metropolis(logposterior_monod,
                                              sample_proposal,
                                              par;
                                              niter = 5_000,
                                              nburnin = 0);

chn2 = Chains(samples2, labels(par))
plot(chn2)
corner(chn2)
```

Much better!

#### Residual Diagnostics

First we compute the residuals with the 'best' parameters
```{julia, results=FALSE}
# get maximum of posterior parameters
par_max = samples2[argmax(lp2)]

# compute residuals
residuals = monod_data.r .- model_monod(monod_data.C, par_max)
```

Plotting the residuals. We hope not to find any structure, but this
model is clearly not perfect:
```{julia, out.width = '80%'}
p = scatter(monod_data.r, residuals, label=false, xlab="r", ylab="residual");
hline!(p, [0], label=false)
```

Quantile-quantile plots are great to see if the residuals are
normal distributed (which was out assumption for the likelihood
functions). Here we see that we have too heavy tails.
```{julia, out.width = '80%'}
qqnorm(residuals, qqline = :R)               # a bit heavy-tailed
```

We also assumed independence of the observations, i.e. we should not
see any auto-correlation. This looks good here:
```{julia, out.width = '80%'}
using StatsBase
acf = StatsBase.autocor(residuals);
bar(0:(length(acf)-1), acf, label=false, xlab="lag", ylab="correlation")
```

### Python

#### Read data

```{python}
from models import model_monod, model_growth
import seaborn as sns
```

```{python}
# Loading data
# Specify the path to the CSV file
file_path = r"../../data/model_monod_stoch.csv"

# Load the CSV file into a pandas DataFrame
data_monod = pd.read_csv(file_path, sep=" ")

# Assuming `data_monod` is a pandas DataFrame that contains columns 'C' and 'r'
# Plot 'C' against 'r'
plt.figure(figsize=(10, 6))
plt.scatter(data_monod['C'], data_monod['r'])

# Set x-axis label
plt.xlabel("C")

# Set y-axis label
plt.ylabel("r")

# Set title
plt.title('"model_monod_stoch.csv"')

# Optimize the plot area and visualization
plt.tight_layout()

# Display the plot
plt.show()
```

#### Define likelihood, prior, and posterior
```{python}
import numpy as np
import pandas as pd
from scipy.stats import lognorm, norm
import warnings

# Suppress all warnings
warnings.filterwarnings("ignore")

# Define the prior mean and standard deviation
prior_monod_mean = 0.5 * np.array([5, 3, 0.5])  # r_max, K, sigma
prior_monod_sd = 0.5 * prior_monod_mean

def logprior_monod(par, mean, sd):
    """
    Log-prior function for the Monod model.

    Parameters:
    - par: array-like, the parameters to evaluate the prior (e.g., ['r_max', 'K', 'sigma'])
    - mean: array-like, the mean values for the parameters
    - sd: array-like, the standard deviation values for the parameters

    Returns:
    - log_prior: the sum of log-prior values calculated from the log-normal distribution
    """
    var = (sd**2) / (mean**2)
    sdlog = np.sqrt(np.log(1 + var))
    meanlog = np.log(mean) - 0.5 * sdlog**2
    log_prior = np.sum(lognorm.logpdf(par, s=sdlog, scale=np.exp(meanlog)))
    return log_prior

def loglikelihood_monod(y, C, par):
    """
    Log-likelihood function for the Monod model.

    Parameters:
    - y: observed growth rates
    - C: substrate concentrations
    - par: array-like, the parameters ('r_max', 'K', 'sigma')

    Returns:
    - log_likelihood: the sum of log-likelihood values
    """
    y_det = model_monod(par, C)
    log_likelihood = np.sum(norm.logpdf(y, loc=y_det, scale=par[2]))
    return log_likelihood

def logposterior_monod(par, data, prior_mean, prior_sd):
    """
    Log-posterior function for the Monod model.

    Parameters:
    - par: array-like, the parameters ('r_max', 'K', 'sigma')
    - data: pandas DataFrame, the data containing observed growth rates and substrate concentrations
    - prior_mean: array-like, the mean values for the parameters
    - prior_sd: array-like, the standard deviation values for the parameters

    Returns:
    - log_posterior: the log posterior probability
    """
    lp = logprior_monod(par, prior_mean, prior_sd)
    if np.isfinite(lp):
        log_posterior = lp + loglikelihood_monod(data['r'], data['C'], par)
        return log_posterior
    else:
        return -np.inf
```

#### Write your own chain function
```{python, results = 'hide'}
def run_mcmc_monod(logposterior_func, data, prior_mean, prior_sd, par_init, sampsize=5000, scale=np.diag([1, 1, 1])):
    """
    Run MCMC for the model.

    Parameters:
    - logposterior_func: function, the log-posterior function to use
    - data: pandas DataFrame, the data containing observed growth rates and substrate concentrations
    - prior_mean: array-like, the mean values for the parameters
    - prior_sd: array-like, the standard deviation values for the parameters
    - par_init: array-like, the initial parameter values
    - sampsize: int, the number of samples to draw (default: 5000)
    - scale: array-like, the covariance matrix for the proposal distribution

    Returns:
    - chain: array, the chain of parameter samples and their log-posterior values
    - acceptance_rate: float, the acceptance rate of the MCMC run
    """
    chain = np.zeros((sampsize, len(par_init) + 1))
    chain[0, :-1] = par_init
    chain[0, -1] = logposterior_func(par_init, data, prior_mean, prior_sd)
    
    accepted = 0
    for i in range(1, sampsize):
        par_prop = chain[i-1, :-1] + np.random.multivariate_normal(np.zeros(len(par_init)), scale)
        logpost_prop = logposterior_func(par_prop, data, prior_mean, prior_sd)
        acc_prob = min(1, np.exp(logpost_prop - chain[i-1, -1]))
        
        if np.random.rand() < acc_prob:
            chain[i, :-1] = par_prop
            chain[i, -1] = logpost_prop
            accepted += 1
        else:
            chain[i, :] = chain[i-1, :]
    
    acceptance_rate = accepted / sampsize

    
    return chain, acceptance_rate
```

#### Create initial chain

```{python, results = 'hide'}
# Define the initial parameters
par_init = [2.5, 1.5, 0.25]

# Run the chain
chain, acceptance_rate = run_mcmc_monod(logposterior_func=logposterior_monod, data=data_monod, 
                                             prior_mean=prior_monod_mean, prior_sd=prior_monod_sd, 
                                             par_init=par_init, sampsize=5000, 
                                             scale=np.diag([1, 1, 1]))
```

```{python}
# Convert the results into a dataframe
df_chain = pd.DataFrame(chain, columns=['r_max', 'K', 'sigma', 'logposterior']) # this is useful for plotting

# Visualize the results
df_chain.describe()
```

The acceptance rate is very low:
```{python}
print("Mean acceptance fraction: {:.3f}".format(acceptance_rate))
```

The poor mixing is also visible from the chain plots:
```{python}
# Plot trace plots
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 10))

parameters = ['r_max', 'K', 'sigma']
for i, param in enumerate(parameters):
    axes[i, 0].plot(df_chain[param])
    axes[i, 0].set_title(f'Trace plot of {param}')
    axes[i, 0].set_xlabel('Iteration')
    axes[i, 0].set_ylabel(param)
    axes[i, 1].hist(df_chain[param], bins=30, density=True)
    axes[i, 1].set_title(f'Histogram of {param}')
    axes[i, 1].set_xlabel(param)
    axes[i, 1].set_ylabel('Density')

plt.tight_layout()
plt.show()
```

Here we see a strong correlation between $K$ and $r_{max}$:
```{python}
# Create pairwise scatter plots between the variables using seaborn's pairplot
pairplot = sns.pairplot(df_chain.iloc[:, :-1])

# Manually adjust the layout if needed
pairplot.fig.tight_layout()

# Show the plots
plt.show()
```

#### Improve jump distribution

```{python, results = 'hide'}
# Calculate the covariance of the samples
sigma_jump = np.cov(df_chain[['r_max', 'K', 'sigma']], rowvar=False) * (1/2)**2

# Run the chain
chain2, acceptance_rate2 = run_mcmc_monod(logposterior_func=logposterior_monod, data=data_monod, 
                                             prior_mean=prior_monod_mean, prior_sd=prior_monod_sd, 
                                             par_init=par_init, sampsize=5000, 
                                             scale=sigma_jump)
                                            
```

Everything looks much better now!
```{python}
print("Mean acceptance fraction: {:.3f}".format(acceptance_rate2))
```

```{python}
# Convert the results into a dataframe
df_chain2 = pd.DataFrame(chain2, columns=['r_max', 'K', 'sigma', 'logposterior']) # this is useful for plotting 

# Visualize the results
df_chain2.describe()
```

```{python}
# Plot trace plots
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 10))

parameters = ['r_max', 'K', 'sigma']
for i, param in enumerate(parameters):
    axes[i, 0].plot(df_chain2[param])
    axes[i, 0].set_title(f'Trace plot of {param}')
    axes[i, 0].set_xlabel('Iteration')
    axes[i, 0].set_ylabel(param)
    axes[i, 1].hist(df_chain2[param], bins=30, density=True)
    axes[i, 1].set_title(f'Histogram of {param}')
    axes[i, 1].set_xlabel(param)
    axes[i, 1].set_ylabel('Density')

plt.tight_layout()
plt.show()
```

#### Residual Diagnostics

First we compute the residuals with the 'best' parameters
```{python}
from scipy import stats

# Get the parameter set that maximizes the log-posterior (As we may have many, we take the mean)
par_max = df_chain2[df_chain2.logposterior==df_chain2.logposterior.max()].mean()[0:2]

# Calculate residuals: observed data minus modeled data using par_max
modeled_data = model_monod(par_max, data_monod['C'])

residuals = data_monod['r'] - modeled_data
```

Plotting the residuals. We hope not to find any structure, but this
model is clearly not perfect:
```{python}
# Plot observed data (data_monod_r) versus residuals
plt.figure(figsize=(8, 4))
plt.scatter(data_monod['r'], residuals, alpha=0.7)
plt.xlabel("r")  # x-axis label
plt.ylabel("Residuals")  # y-axis label
plt.title("Observed Data vs. Residuals")

# Add a horizontal line at y = 0
plt.axhline(y=0, color='red', linestyle='--')

# Show the plot
plt.tight_layout()
plt.show()
```

Quantile-quantile plots are great to see if the residuals are
normal distributed (which was out assumption for the likelihood
functions). Here we see that we have too heavy tails.
```{python}
# Create a figure and axis for the plot
plt.figure(figsize=(8, 4))

# Generate a Q-Q plot using scipy's probplot function
# probplot returns a tuple with the results and a dictionary with the plot data
stats.probplot(residuals, dist="norm", plot=plt)

# Add labels and title
plt.xlabel("Theoretical Quantiles")
plt.ylabel("Sample Quantiles")
plt.title("Q-Q Plot of Residuals")

# Show the plot
plt.tight_layout()
plt.show()
```

We also assumed independence of the observations, i.e. we should not
see any auto-correlation. This looks good here:
```{python}
from statsmodels.tsa.stattools import acf

# Calculate the autocorrelation function (ACF) of the residuals
acf_values = acf(residuals, nlags=40, fft=True)

# Create a figure and axis for the plot
plt.figure(figsize=(8, 6))

# Plot the ACF values using a bar plot
plt.stem(range(len(acf_values)), acf_values)
plt.xlabel("Lag")
plt.ylabel("Autocorrelation")
plt.title("Autocorrelation Function (ACF) of Residuals")

# Add a horizontal line at y = 0 for reference
plt.axhline(y=0, color='gray', linestyle='--')

# Show the plot
plt.tight_layout()
plt.show()
```