## Solution {.tabset}

### Julia

First we make sure, that we can sample in an unlimited space by using a
appropriate transformations:
```{julia}
using TransformVariables
using TransformedLogDensities: TransformedLogDensity
using LogDensityProblemsAD: ADgradient

# defines the 'legal' parameter space, all parameter cannot be negative
# due to the lognormal prior
trans = as((r_max = asℝ₊, K = asℝ₊, sigma = asℝ₊))

# define an object that is compatible with the LogDensityProblems.jl interface.
# This enables compability with idfferent samplers.
lp = TransformedLogDensity(trans, θ -> logposterior_monod(ComponentVector(θ)))

```

We compute the gradient of the logposterior with automatic
differentiation (AD). In Julia we can choose AD different back-ends. `ForwardDiff.jl` is
a good option if we do not have many parameters.

```{julia}
lp = ADgradient(:ForwardDiff, lp)
```

#### HMC

We use a "No-U-turn sampler" HMC sampler, similar to the one
implemented in STAN. Note, that for HMC we typically
need a much lower number of samples.
```{julia}
import Random
using DynamicHMC: mcmc_with_warmup, ProgressMeterReport, Diagnostics.summarize_tree_statistics

# run for 100 samples
par_init = [-1.0, -1.0, -1.0];   # in ℝⁿ

results = mcmc_with_warmup(Random.GLOBAL_RNG, lp, 100;
                           initialization = (q = par_init, ),
                           reporter = ProgressMeterReport());

# some convergence statistics
summarize_tree_statistics(results.tree_statistics)

results.posterior_matrix                     # this are the samples in ℝⁿ !
```

We need to back-transform the samples to the "normal" parameter space. For
 plotting we convert to `MCMCChains.Chains`:
```{julia, out.width = '80%'}
# back-transform samples
_samples = [TransformVariables.transform(trans, s)
            for s in eachcol(results.posterior_matrix)];

samples = vcat((hcat(i...) for i in _samples)...);
chn = MCMCChains.Chains(samples,  [:r_max, :K, :sigma])

plot(chn)
corner(chn[25:end,:,:])
```

#### BarkerMCMC

The same steps are taken for the `BarkerMCMC`:
```{julia, out.width = '80%'}
using BarkerMCMC: barker_mcmc

par_init = [-1.0, -1.0, -1.0];   # note, this is in ℝⁿ

# see `?barker_mcmc` for all options
res = barker_mcmc(lp,
                  par_init;
                  n_iter = 1000,
                  target_acceptance_rate=0.4);
# Transform the samples to the "normal" space and convert to `Chains`
_samples = [TransformVariables.transform(trans, s)
            for s in eachrow(res.samples)];
samples = vcat((hcat(i...) for i in _samples)...);
chn = Chains(samples,  [:r_max, :K, :sigma])

plot(chn)
corner(chn[250:end,:,:])
```
