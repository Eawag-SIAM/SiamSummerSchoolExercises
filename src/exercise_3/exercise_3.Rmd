---
title: "Exercise 3: Adaptive Monte Carlo Markov Chain samplers"
author: "Eawag Summer School in Environmental Systems Analysis"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    number_sections: no
    toc: TRUE
    toc_depth: 2
    toc_float: true
params:
  showsolutions: TRUE
---


<!-- setup Julia environmental and JS Code -->
```{r, child = "../header.Rmd"}
```


In this exercise we will look at different adaptive Metropolis
algorithms. The aim is to get an intuition for what their
"adaptations" are doing, by examining testing the behaviour with our
simple `monod` model. You are encouraged to play with each of the
adaptation parameters of each algorithm and to check how it influences
the resulting sample chain.

### Define the posterior distribution {.tabset}

You can use you own code from exercises 2 or the functions defined
below. Of course you can also do the exercises with the growth model.

#### R

```{r, results=FALSE}
## load the monod model
source("../../models/models.r")

## read data
data.monod <- read.table("../../data/model_monod_stoch.csv", header=T)

## Logprior for model "monod": lognormal distribution

prior.monod.mean <- 0.5 * c(r_max=5, K=3, sigma=0.5)
prior.monod.sd   <- 0.5 * prior.monod.mean

logprior.monod <- function(par, mean, sd){
    sdlog <- sqrt(log(1+sd*sd/(mean*mean)))
    meanlog <- log(mean) - sdlog*sdlog/2
    return(sum(dlnorm(par, meanlog=meanlog, sdlog=sdlog, log=TRUE)))
}

## Log-likelihood for model "monod"
loglikelihood.monod <- function(y, C, par){

    ## deterministic part:
    y.det <- model.monod(par, C) # defined in `models.r`

    ## Calculate loglikelihood assuming independence:
    return( sum(dnorm(y, mean=y.det, sd=par['sigma'], log=TRUE )) )
}

## Log-posterior for model "monod"
logposterior.monod <- function(par) {
    lp <- logprior.monod(par, prior.monod.mean, prior.monod.sd)
    if(is.finite(lp)){
        return( lp + loglikelihood.monod(data.monod$r, data.monod$C, par) )
    } else {
        return(-Inf)
    }
}
```

#### Julia


```{julia, results=FALSE}
using DataFrames
import CSV
using Distributions
using ComponentArrays

monod_data = CSV.read("../../data/model_monod_stoch.csv", DataFrame)

## load monod model
include("../../models/models.jl");

## read data
monod_data = CSV.read("../../data/model_monod_stoch.csv", DataFrame)

# set parameters
prior_monod_mean = ComponentVector(r_max = 2.5, K=1.4, sigma=0.25);
prior_monod_sd = 0.5 .* prior_monod_mean;

## Use a lognormal distribution for all model parameters
function logprior_monod(par, m, sd)
    μ = @. log(m/sqrt(1+sd^2/m^2))
    σ = @. sqrt(log(1+sd^2/m^2))
    return sum(logpdf.(LogNormal.(μ, σ), par)) # sum because we are in the log-space
end

## Log-likelihood for model "monod"
function loglikelihood_monod(par::ComponentVector, data::DataFrame)
    y_det = model_monod(data.C, par)
    return sum(logpdf.(Normal.(y_det, par.sigma), data.r))
end

## Log-posterior for model "monod"
function logposterior_monod(par::ComponentVector)
    lp = logprior_monod(par, prior_monod_mean, prior_monod_sd)
    if !isinf(lp)
        lp += loglikelihood_monod(par, monod_data)
    end
    lp
end
```


## 1. Adaptive Metropolis with delayed rejection {.tabset}

The adaptive Metropolis with delayed rejection, as described by
Haario.

### R

It is implemented in the function `modMCMC` of the package
`FME`. Note, this function expects the _negative_ log density",
which is sometimes called _energie_. See the `modMCMC` function documentation for details.


```r
neg.log.post <- function(par) -logposterior.monod(par)
```
Now call the ```modMCMC``` function and investigate the effects of the ```updatecov``` parameter (try values of 10, 100 and 1000), which determines how often the covariance of the jump distribution is updated, and the ```ntrydr``` parameter (try values of 1, 3 and 10), which determines the number of permissible jump attempts before the step is rejected. In particular, examine the first part of the chain and see how the adaptation works.

```r
AMDR <- modMCMC(
    f         = neg.log.post,
    p         = par.init,
    jump      = jump.cov,
    niter     = 10000,
    updatecov = 10,
    ntrydr    = 3
)
```

- What could you use as initial values `par.init`?

- What is a meaningful covariance matrix for the jump distribution?

- Plot the chains and see how quick the convergence is.

- Look at 2d marginal plots (you may use the function `ipairs` from
  package `IDPmisc`). What happens in these marginal plots if you don't cut off a burn-in or only use the beginning of the chain?


### Julia

TODO


## 2. Robust adaptive Metropolis {.tabset}

The robust adaptive Metropolis algorithm by proposed by Vihola is
often a good choice. It adapts the scale and rotation of the
covariance matrix until it reached a predefined acceptance rate.

### R

The package `adaptMCMC` which provides the function `MCMC`. If the
parameter `adapt` is set to `TRUE` it implements the adaptation
propsoed by Vihola. Again examine the effect of the adaptation
settings on the chains, in particular examining the first part of the
chain to see how the adaptation works. Here the adaptation is
determined by parameter ```acc.rate```. Try values between 0.1 and 1.

How is the influence on the burn-in? What happens, if you use a very
bad initial value?

```r
RAM   <- MCMC(
    p        = logposterior.monod,
    n        = 10000,
    init     = par.start,
    scale    = jump.cov,
    adapt    = TRUE,
    acc.rate = 0.5
)

str(RAM)
```


- Did the algorithm reach the desired acceptance rate?

- How is the covariance matrix after the adaptation different from the
  initial covariance that you provided?


### Julia
TODO


## 3. Population based sampler {.tabset}

Population based sampler do not have an explicit jump
distribution. Instead, the run multiple chain (often called _particles_
or _walkers_ in this context) in parallel and the proposals are generated based on
the position of the other particles.

### R

The  Affine-Invariant MCMC population sampler proposed Goodman and
Weare

Population based samplers are is implemented in the package
`mcmcensemble` as `MCMCEnsemble`. It has two methods: _stretch move_
(`method = "stretch"`) (the  Affine-Invariant MCMC population
sampler proposed Goodman and Weare) and _differential evolution_
(`method = "differential.evolution"`) proposed by ter Braak and
Vrugt.


```r

EMCEE <- MCMCEnsemble(
    f           = logposterior.monod,
    lower.inits = par.start.lower,
    upper.inits = par.start.upper,
    max.iter    = 10000,
    n.walkers   = n.walkers,
    method      = "stretch",
    coda        = FALSE
)
```

- How do you define `par.start.lower` and `par.start.upper`?

- What is the influence of the number of walkers?

- Which method works better in this case?


### Julia


## 4. TODO Comparing different chains (optional)

Once you have generated chains for each of the methods, you may wish
to compare them against one another.

- compare marginals

- compare burn-in length

- compare effective sampling size

## 5. TODO Gradient based samplers (Julia only)

If we are able to compute the gradient of the log density, $\nabla
\log p$, we can use much more efficient sampling methods. For small
number of parameters this may not be relevant, for lager problems
(more than 20 dimension) the differences can be huge.

Julia is particularly well suited for this applications, because many
libraries for Automatic Differentiation (AD) are available - methods
that compute the gradient by analyzing the code.

- HMC, NUTS
- BarkerMCMC
