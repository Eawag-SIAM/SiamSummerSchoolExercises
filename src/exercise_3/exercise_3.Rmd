---
title: "Exercise 3: Adaptive Monte Carlo Markov Chain samplers"
author: "Eawag Summer School in Environmental Systems Analysis"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    number_sections: no
    toc: TRUE
    toc_depth: 2
    toc_float: true
params:
  showsolutions: TRUE
---


<!-- setup Julia environmental and JS Code -->
```{r, child = "../header.Rmd"}
```


In this exercise we will look at different adaptive Metropolis
algorithms. The aim is to get an intuition for what their
"adaptations" are doing, by examining testing the behaviour with our
simple `monod` model. You are encouraged to play with each of the
adaptation parameters of each algorithm and to check how it influences
the resulting sample chain.

# Define posterior distribution {.tabset}

You can use you own code from exercises 2 or the functions defined
below. And of course you can also do the exercises with the growth model.

## R

```{r, results=FALSE}
## load the monod model
source("../../models/models.r")

## read data
data.monod <- read.table("../../data/model_monod_stoch.csv", header=T)

## Logprior for model "monod": lognormal distribution
prior.monod.mean <- 0.5 * c(r_max=5, K=3, sigma=0.5)
prior.monod.sd   <- 0.5 * prior.monod.mean

logprior.monod <- function(par, mean, sd){
    sdlog <- sqrt(log(1+sd*sd/(mean*mean)))
    meanlog <- log(mean) - sdlog*sdlog/2
    return(sum(dlnorm(par, meanlog=meanlog, sdlog=sdlog, log=TRUE)))
}

## Log-likelihood for model "monod"
loglikelihood.monod <- function(y, C, par){

    ## deterministic part:
    y.det <- model.monod(par, C) # defined in `models.r`

    ## Calculate loglikelihood assuming independence:
    return( sum(dnorm(y, mean=y.det, sd=par['sigma'], log=TRUE )) )
}

## Log-posterior for model "monod"
logposterior.monod <- function(par) {
    lp <- logprior.monod(par, prior.monod.mean, prior.monod.sd)
    if(is.finite(lp)){
        return( lp + loglikelihood.monod(data.monod$r, data.monod$C, par) )
    } else {
        return(-Inf)
    }
}
```

## Julia


```{julia, results=FALSE}
using DataFrames
import CSV
using Distributions
using ComponentArrays

monod_data = CSV.read("../../data/model_monod_stoch.csv", DataFrame)

## load monod model
include("../../models/models.jl");

## read data
monod_data = CSV.read("../../data/model_monod_stoch.csv", DataFrame)

# set parameters
prior_monod_mean = ComponentVector(r_max = 2.5, K=1.4, sigma=0.25);
prior_monod_sd = 0.5 .* prior_monod_mean;

## Use a lognormal distribution for all model parameters
function logprior_monod(par, m, sd)
    μ = @. log(m/sqrt(1+sd^2/m^2))
    σ = @. sqrt(log(1+sd^2/m^2))
    return sum(logpdf.(LogNormal.(μ, σ), par)) # sum because we are in the log-space
end

## Log-likelihood for model "monod"
function loglikelihood_monod(par::ComponentVector, data::DataFrame)
    y_det = model_monod(data.C, par)
    return sum(logpdf.(Normal.(y_det, par.sigma), data.r))
end

## Log-posterior for model "monod"
function logposterior_monod(par::ComponentVector)
    lp = logprior_monod(par, prior_monod_mean, prior_monod_sd)
    if !isinf(lp)
        lp += loglikelihood_monod(par, monod_data)
    end
    lp
end
```


# 1. Adaptive Metropolis with delayed rejection ★

We try the adaptive Metropolis with delayed rejection as described by
([Haario, Saksman and Tamminen
(2001)](https://projecteuclid.org/euclid.bj/1080222083).


- What could you use as initial values?

- What is a meaningful initial covariance matrix for the jump distribution?

- Plot the chains and see how quick the convergence is.

- Look at 2d marginal plots. What happens in these marginal plots if you don't cut off a burn-in or only use the beginning of the chain?

## Hints {.tabset}
### R

It is implemented in the function
[`modMCMC`](https://www.rdocumentation.org/packages/FME/versions/1.3.6.2/topics/modMCMC)
of the package
[`FME`](https://cran.r-project.org/web/packages/FME/index.html). Note,
this function expects the _negative_ log density", which is sometimes
called _energie_. See the `modMCMC` function [documentation](https://www.rdocumentation.org/packages/FME/versions/1.3.6.2/topics/modMCMC) for
details.


```r
neg.log.post <- function(par) -logposterior.monod(par)
```
Now call the ```modMCMC``` function and investigate the effects of the ```updatecov``` parameter (try values of 10, 100 and 1000), which determines how often the covariance of the jump distribution is updated, and the ```ntrydr``` parameter (try values of 1, 3 and 10), which determines the number of permissible jump attempts before the step is rejected. In particular, examine the first part of the chain and see how the adaptation works.

```r
AMDR <- modMCMC(
    f         = neg.log.post,
    p         = par.init,
    jump      = jump.cov,
    niter     = 10000,
    updatecov = 10,
    ntrydr    = 3
)
```

### Julia

The package
[`AdaptMCMC`](https://mvihola.github.io/docs/AdaptiveMCMC.jl/)
provides multiple MCMC algorithms including the adaptive Metropolis.

```Julia
using ComponentArrays
using AdaptiveMCMC
using MCMCChains
using Plots
using StatsPlots

θinit = ComponentVector(r_max=..., K=..., sigma=...)

res = adaptive_rwm(θinit, logposterior_monod,
                   10_000;
                   algorithm = :am,
                   b=1,   # length of burn-in. Set to 1 for no burn-in.
                   );

# convert to MCMCChains for summary and plotting
chn = Chains(res.X', labels(θinit))

plot(chn)
corner(chn)
```


```{r, child = if(params$showsolutions){"exe_3_solution_1.Rmd"}}
```

# 2. Robust adaptive Metropolis ★

The robust adaptive Metropolis algorithm by proposed by [Vihola  (2012)](http://dx.doi.org/10.1007/s11222-011-9269-5) is
often a good choice. It adapts the scale and rotation of the
covariance matrix until it reached a predefined acceptance rate.


- Did the algorithm reach the desired acceptance rate?

- How is the covariance matrix after the adaptation different from the
  initial covariance that you provided?

## Hints {.tabset}
### R

The package [`adaptMCMC`](https://cran.r-project.org/web/packages/adaptMCMC/index.html) which provides the function `MCMC`. If the
parameter `adapt` is set to `TRUE` it implements the adaptation
propsoed by Vihola. Again examine the effect of the adaptation
settings on the chains, in particular examining the first part of the
chain to see how the adaptation works. Here the adaptation is
determined by parameter `acc.rate`. Try values between 0.1 and 1.

How is the influence on the burn-in? What happens, if you use a very
bad initial value?

```r
RAM <- MCMC(
    p        = logposterior.monod,
    n        = 10000,
    init     = par.start,
    scale    = jump.cov,
    adapt    = TRUE,
    acc.rate = 0.5
)

str(RAM)
```

### Julia
The package
[`AdaptMCMC`](https://mvihola.github.io/docs/AdaptiveMCMC.jl/)
provides multiple MCMC algorithms including the robust adaptive Metropolis.

```Julia
using ComponentArrays
using AdaptiveMCMC
using MCMCChains
using Plots
using StatsPlots

θinit = ComponentVector(r_max=..., K=..., sigma=...)

res = adaptive_rwm(θinit, logposterior_monod,
                   10_000;
                   algorithm = :ram,
                   b=1,   # length of burn-in. Set to 1 for no burn-in.
                   );

# convert to MCMCChains for summary and plotting
chn = Chains(res.X', labels(θinit))

plot(chn)
corner(chn)
```


```{r, child = if(params$showsolutions){"exe_3_solution_2.Rmd"}}
```

# 3. Population based sampler ★

Population based sampler do not have an explicit jump
distribution. Instead, the run multiple chain (often called
_particles_ or _walkers_ in this context) in parallel and the
proposals are generated based on the position of the other particles.

A popular algorithm of this class is the Affine-Invariant MCMC
population sampler proposed by [Goodman and Weare
(2010)](https://msp.org/camcos/2010/5-1/camcos-v5-n1-p04-p.pdf).  The
algorithm is often called _EMCEE_ based on the python package. with
the same name.

## Hints {.tabset}
### R

Population based samplers are is implemented in the package
`mcmcensemble` as `MCMCEnsemble`. It has two methods: _stretch move_
(`method = "stretch"`) (the  Affine-Invariant MCMC population
sampler proposed Goodman and Weare) and _differential evolution_
(`method = "differential.evolution"`) proposed by ter Braak and
Vrugt.


```r
EMCEE <- MCMCEnsemble(
    f           = logposterior.monod,
    lower.inits = par.start.lower,
    upper.inits = par.start.upper,
    max.iter    = 10000,
    n.walkers   = n.walkers,
    method      = "stretch",
    coda        = FALSE
)
```

- How do you choose `par.start.lower` and `par.start.upper`? Better
  wide or narrow?

- What is the influence of the number of walkers?

- Which method works better in this case?


### Julia

We use the package [`KissMCMC`](https://github.com/mauro3/KissMCMC.jl) which provides a function `emcee`:

```julia
using ComponentArrays
using KissMCMC: emcee
using Plots
using StatsPlots

# number of walkers (parallel chains)
n_walkers = 10

## We need a vector inital calues, one for each walkers.
## Make sure tha they do not start all at the same point.
θinits = [θinit .* rand(3) for _ in 1:n_walkers]

# Run sampler
samples, acceptance_rate, lp = emcee(logposterior_monod,
                                     θinits;
                                     niter = 10_000, # total number of density evaluations
                                     nburnin = 0);

# This looks a bit ugly. It just converts the result into
# a `MCMCChains.Chains` object for plotting.
X = permutedims(
    cat((hcat(samples[i]...) for i in 1:n_walkers)..., dims=3),
    [2, 1, 3]);
chn = Chains(X, labels(θinit))

# plotting
plot(chn)
corner(chn)
```

- How do you define the initial values? Very similar or very different?

- What is the influence of the number of walkers?


```{r, child = if(params$showsolutions){"exe_3_solution_3.Rmd"}}
```

# 4. TODO Comparing different chains (optional)

Once you have generated chains for each of the methods, you may wish
to compare them against one another.

- compare marginals

- compare burn-in length

- compare effective sampling size


```{r, child = if(params$showsolutions){"exe_3_solution_4.Rmd"}}
```

# 5. Gradient based samplers ⚡

If we are able to compute the gradient of the log density, $\nabla
\log p$, we can use much more efficient sampling methods. For small
number of parameters this may not be relevant, for lager problems
(more than 20 dimension) the differences can be huge.

Julia is particularly well suited for this applications, because many
libraries for Automatic Differentiation (AD) are available - methods
that compute the gradient of (almost) any Julia function by analyzing
the code.

Because there is no equally powerful AD available in R, we can do this
exercise only with Julia.


## Parameter transformation

Most gradient based sampler assume that all parameters are in
$\mathbb{R}$. If we have a parameter that exist in a limited space,
such as a standard deviation that is never negative, we need to
transform the model parameter before sampling. For this we need three
ingredients:

- a function that maps every vector in $\mathbb{R}^n$ to our "normal"
	model parameter space,

-  the inverse of this function,

-  the determinate of the Jacobian of this functions.

The package
[TransformVariables](https://github.com/tpapp/TransformVariables.jl)
helps us with this transformations. We need a function that takes a
 vector in $\mathbb{R}^n$ to evaluates the posterior.

```julia
using TransformVariables

# defines the 'legal' parameter space, all parameter cannot be negative
# due to the lognormal prior
trans = as((r_max = asℝ₊, K = asℝ₊, sigma = asℝ₊))

# define a function that takes a parameter vector in ℝ^n
function logposterior_monod_Rn(par_Rn)
    # transforms from ℝ^n to parameter space,
    # and compute log of the determinante of the jacobian
    par, logjac = TransformVariables.transform_and_logjac(trans, par_Rn)

    # do not forget to add the log determinante!
    logposterior_monod(ComponentVector(par)) + logjac
end
```

We can now sample in $\mathbb{R}^n$ and later transform the samples
in the model parameter space with:
```julia
TransformVariables.transform(trans, [-1,-1,-1]) # -> (r_max=0.367, K=0.367, sigma=0.367)
```

## Automatic Differentation

The last ingredient we need is a function that computes the gradient
of `logposterior_monod_Rn`. To do so we need to differentiate through
our model, the likelihood, the prior, the parameter transformation,
and the determinate of the Jaccobian. Needles to say, even for our
very simple model this would be very tedious to do manually!

Instead we use  [Automatic Differentiation
(AD)](https://en.wikipedia.org/wiki/Automatic_differentiation). Julia
has [multiple packages](https://juliadiff.org/) for AD that make
different trade-offs. We use
[`ForwardDiff`](https://github.com/JuliaDiff/ForwardDiff.jl) that is
well suited for smaller dimensions.

For AD requires that all your model code is implemented
in pure Julia! Otherwise there are few restrictions. For example, you
can compute the gradient of the _growth_ model even though it uses an
advances adaptive ODE solver internally.

```Julia
import ForwardDiff

# derive a function that computes the gradient
∇logposterior_monod_Rn(par_Rn) = ForwardDiff.gradient(logposterior_monod_Rn, par_Rn)

# make sure that you can run both functions for all values in ℝ^n
par_test = randn(3)
logposterior_monod_Rn(par_test)
∇logposterior_monod_Rn(par_test)
```


## Hamiltonian Monte Carlo

Hamiltonian Monte Carlo (HMC) is one of the most powerful methods to sample
in high dimensions. The "No-U-Turn Sampler" (NUTS) is a popular versions, for
example it is used in [_STAN_](https://mc-stan.org/).

The package
[`AdvancedHMC`](https://github.com/TuringLang/AdvancedHMC.jl) provides
 building blocks that can be combined to construct various HMC samplers. The
 function is a wrapper that provides a NUTS sampler as implemented in STAN.


`````julia
using AdvancedHMC

"""
# NUTS HMC sampler as used by STAN
Based on the documentation of AdvancedHMC.jl

```Julia
stanHMC(lp::Function, ∇lp::Function,  θ_init;
        n_samples::Int=1000, n_adapts::Int=n_samples÷2)
```

### Arguments
- `lp`:  log density to sample from (up to a constant)
- `∇lp`: define how to get the log gradient of `lp`.
- `n_samples::Int=1000`: number of samples
- `n_adapts::Int=n_samples÷2`: length of adaptation

### Return Value
A named tuple with fields:
- `samples`: array containing the samples
- `stats`: contains various statistics form the sampler. See AdvancedHMc documentation.
"""
function stanHMC(lp::Function, ∇lp::Function,  θ_init,;
                 n_samples::Int=1000, n_adapts::Int=n_samples÷2)

    # Define a Hamiltonian system
    D = length(θ_init)   # number of parameters
    metric = DiagEuclideanMetric(D)
    hamiltonian = Hamiltonian(metric, lp, θr -> (lp(θr), ∇lp(θr)))

    # Define a leapfrog solver, with initial step size chosen heuristically
    initial_ϵ = find_good_stepsize(hamiltonian, θ_init)
    integrator = Leapfrog(initial_ϵ)

    # Define an HMC sampler, with the following components
    #   - multinomial sampling scheme,
    #   - generalised No-U-Turn criteria, and
    #   - windowed adaption for step-size and diagonal mass matrix
    proposal = NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)
    adaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))

    # -- run sampler
    samples, stats = sample(hamiltonian, proposal, θ_init, n_samples,
                            adaptor, n_adapts; progress=true)

    return (samples=samples, stats=stats)

end
`````

HMC sampler need many density and gradient evaluations to produce a
single proposal. However, the acceptance rate of a HMC sampler should
be close to one.  We can use the wrapper function `stanHMC` like this:
```julia
par_init = [-1.0, -1.0, -1.0]   # in ℝⁿ
res = stanHMC(logposterior_monod_Rn,
              ∇logposterior_monod_Rn,
              par_init;
              n_samples = 100);

res.samples                     # this are the samples in ℝⁿ !
```

The samples we get are in $\mathbb{R}^n$. Before we have a look, let's
transform them in the "normal" parameter space:
```julia
# Transform the samples to the "normal" space and convert to `Chains`
_chn = [ComponentVector(TransformVariables.transform(trans, res.samples[i]))
        for i in 1:size(res.samples, 1)];
chn = Chains(_chn,  [:r_max, :K, :sigma])

plot(chn)
corner(chn)
```


## BarkerMCMC

[Livingstone and  Zanella (2021)]( https://doi.org/10.1111/rssb.12482)
proposed a comparable simple MCMC algorithm that uses the gradient to adapt the
jump distribution. It is a promising alternative to HMC in
cases the number of parameters is not very high, or if the gradient is
expected to be noisy (which is often the case if a model uses adaptive
ODE solvers).

[`BarkerMCMC.jl`](https://github.com/scheidan/BarkerMCMC.jl) implements
it including an adaptation that aims at a given acceptance rate.


```julia
using BarkerMCMC: barker_mcmc

par_init = [-1.0, -1.0, -1.0]   # in ℝⁿ

# see `?barker_mcmc` for all options
res = barker_mcmc(logposterior_monod_Rn,
                  ∇logposterior_monod_Rn,
                  par_init;
                  n_iter = 1000,
                  target_acceptance_rate=0.4);

res.samples                     # this are the samples in ℝⁿ !
res.log_p
```
The samples we get are in $\mathbb{R}^n$. Before we have a look, let's
transform them in the "normal" parameter space:
```julia
using MCMCChains
using StatsPlots

# Transform the samples to the "normal" space and convert to `Chains`
_chn = [ComponentVector(TransformVariables.transform(trans, res.samples[i,:]))
        for i in 1:size(res.samples, 1)];
chn = Chains(_chn,  [:r_max, :K, :sigma])

plot(chn)
corner(chn)
```


```{r, child = if(params$showsolutions){"exe_3_solution_5.Rmd"}}
```
